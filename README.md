ðŸ¤– AI Chatbot Backend (Python + Ollama + TTS)
--
This project uses a fully open-source AI stack to provide:

ðŸ’¬ Streaming AI chat replies

ðŸŽ¤ Voice output (Text-to-Speech)

ðŸ§  Local LLM via Ollama

ðŸŒ FastAPI backend

ðŸ“¦ Requirements
--


Ollama (Local LLM Server)
--
Install Ollama:

- ðŸ‘‰ https://ollama.com

After installing, pull a model:

  - ollama pull llama3


Start Ollama server:

  - ollama serve

  - (Default runs on http://localhost:11434)



 Python Dependencies
--
Python (Recommended)
Python 3.11.x -> (Required for TTS compatibility)

Install required packages:

- pip install fastapi uvicorn requests TTS

ðŸš€ Running the Backend

Start FastAPI server:

- python -m uvicorn main:app --reload

Backend will run on:

- http://127.0.0.1:8000

ðŸ” Chat Streaming Endpoint
- POST /chat-stream


- Streams AI response from Ollama in real-time.

ðŸ”Š Voice (Text-to-Speech)

- This project uses Coqui TTS (open-source) for natural AI voice.

Example model:

- tts_models/en/ljspeech/glow-tts


Voice is generated after full AI message is received.

- ðŸŒ Web Support (CORS Enabled)

- FastAPI is configured with CORS to support.

ðŸ§  Stack Overview
--
- FastAPI	-> Backend server
- Ollama	-> Local LLM (AI brain)
- Coqui TTS ->	Voice generation
- Flutter	 -> Frontend UI (https://github.com/darttechwala/ChatBot)

âœ… Features
--
- Fully offline capable (local AI)

- Streaming chat responses

- Voice replies

- Multi-platform (Android, iOS, Web, macOS, Windows)

- Open-source stack

âš  Notes
--

Ollama must be running before starting backend
